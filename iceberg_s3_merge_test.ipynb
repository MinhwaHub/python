{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 16:53:34 WARN Utils: Your hostname, minaui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.9.60 instead (on interface en7)\n",
      "25/05/26 16:53:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/05/26 16:53:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "\n",
    "# AWS 세션을 사용하여 자격 증명 가져오기\n",
    "session = boto3.Session(profile_name=\"dev-mina\")\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "# 액세스 키, 비밀 키, 세션 토큰 가져오기\n",
    "aws_access_key = credentials.access_key\n",
    "aws_secret_key = credentials.secret_key\n",
    "aws_token = credentials.token\n",
    "\n",
    "# S3 리소스를 생성하여 S3 객체에 접근\n",
    "s3_resource = session.resource(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    aws_session_token=aws_token,  \n",
    ")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "jars_path = \"/Users/mina/nx-mina/test\"  # JAR 파일들이 있는 디렉토리\n",
    "jars = [\n",
    "    f\"{jars_path}/hadoop-aws-3.3.1.jar\",\n",
    "    f\"{jars_path}/aws-java-sdk-bundle-1.12.781.jar\",\n",
    "    f\"{jars_path}/iceberg-spark-runtime-3.5_2.12-1.9.0.jar\",\n",
    "    f\"{jars_path}/spark-avro_2.12-3.5.5.jar\", #spark version과 맞추기 \n",
    "]\n",
    "jars_str = \",\".join(jars)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .config(\"spark.jars\", jars_str) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.session.token\", aws_token) \\\n",
    "    .config(\"spark.sql.catalog.s3cat\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.s3cat.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.s3cat.warehouse\", \"s3a://emr-data-pipeline-test/iceberg-warehouse/\") \\\n",
    "    .config(\"spark.sql.catalog.s3cat.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.sql.catalog.s3cat.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spark-submit --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 16:37:21 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/05/26 16:37:22 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/\n",
      "You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.\n",
      "This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.\n",
      "The AWS SDK for Java 1.x is being used here:\n",
      "at java.base/java.lang.Thread.getStackTrace(Thread.java:1602)\n",
      "at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)\n",
      "at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)\n",
      "at com.amazonaws.internal.EC2ResourceFetcher.<clinit>(EC2ResourceFetcher.java:44)\n",
      "at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.<init>(InstanceMetadataServiceCredentialsFetcher.java:38)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:111)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:91)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:75)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<clinit>(InstanceProfileCredentialsProvider.java:58)\n",
      "at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.initializeProvider(EC2ContainerCredentialsProviderWrapper.java:66)\n",
      "at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.<init>(EC2ContainerCredentialsProviderWrapper.java:55)\n",
      "at org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider.<init>(IAMInstanceCredentialsProvider.java:49)\n",
      "at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:756)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:688)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:621)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:737)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:446)\n",
      "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
      "at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "at org.apache.iceberg.hadoop.Util.getFs(Util.java:55)\n",
      "at org.apache.iceberg.hadoop.HadoopCatalog.initialize(HadoopCatalog.java:112)\n",
      "at org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n",
      "at org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n",
      "at org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n",
      "at org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n",
      "at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n",
      "at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n",
      "at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n",
      "at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)\n",
      "at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)\n",
      "at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n",
      "at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n",
      "at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n",
      "at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.CreateTable.mapChildren(v2Commands.scala:447)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n",
      "at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n",
      "at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n",
      "at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "at scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "at scala.collection.immutable.List.foreach(List.scala:431)\n",
      "at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "at py4j.Gateway.invoke(Gateway.java:282)\n",
      "at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "at py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "at java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS s3cat.db.users (\n",
    "        id INT,\n",
    "        name STRING,\n",
    "        email STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 16:38:31 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to iceberg-warehouse/db/users/data/00000-0-d594af11-9a44-4b90-94fa-12402fca49b2-0-00001.parquet. This is unsupported\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO s3cat.db.users VALUES\n",
    "    (1, 'Alice', 'alice@example.com'),\n",
    "    (2, 'Bob', 'bob@example.com')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------------+\n",
      "| id| name|            email|\n",
      "+---+-----+-----------------+\n",
      "|  1|Alice|alice@example.com|\n",
      "|  2|  Bob|  bob@example.com|\n",
      "+---+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM s3cat.db.users\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "S3에 데이터는 .avro 파일로 저장됨\n",
    "Apache Avro는 Apache Hadoop의 데이터 직렬화 프레임워크이며 데이터 + 스키마가 함께 저장되므로 스키마가 없는 환경에서도 읽을 수 있는 자체 기술서 포맷임\n",
    "\n",
    "iceberg-warehouse/db/users/\n",
    "├── data/                    ← row data (.parquet)\n",
    "├── metadata/\n",
    "│   ├── snap-00001-xyz.avro        ← snapshot manifest list\n",
    "│   ├── 00000-abc.avro             ← manifest file (파일별 정보)\n",
    "│   ├── v1.metadata.json           ← 테이블 스키마/이력 전체 요약\n",
    "│   └── version-hint.text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MERGE TEST\n",
    "'''\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# 예제 업데이트 데이터\n",
    "merge_data = [\n",
    "    Row(id=1, name='Alice Updated', email='alice_updated@example.com'),  # 업데이트 대상\n",
    "    Row(id=3, name='Charlie', email='charlie@example.com')               # 새로 삽입될 데이터\n",
    "]\n",
    "\n",
    "merge_df = spark.createDataFrame(merge_data)\n",
    "merge_df.createOrReplaceTempView(\"updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    MERGE INTO s3cat.db.users AS target\n",
    "    USING updates AS source\n",
    "    ON target.id = source.id\n",
    "\n",
    "    WHEN MATCHED THEN\n",
    "    UPDATE SET *\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------------+\n",
      "| id|         name|               email|\n",
      "+---+-------------+--------------------+\n",
      "|  1|Alice Updated|alice_updated@exa...|\n",
      "|  3|      Charlie| charlie@example.com|\n",
      "|  2|          Bob|     bob@example.com|\n",
      "+---+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM s3cat.db.users\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+---------------+--------------------+--------------------+\n",
      "|status|        snapshot_id|sequence_number|file_sequence_number|           data_file|\n",
      "+------+-------------------+---------------+--------------------+--------------------+\n",
      "|     1|9022688425129316256|           NULL|                NULL|{0, s3a://emr-dat...|\n",
      "|     1|9022688425129316256|           NULL|                NULL|{0, s3a://emr-dat...|\n",
      "+------+-------------------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "argo file 읽기\n",
    "'''\n",
    "df = spark.read.format(\"avro\").load(\"s3a://emr-data-pipeline-test/iceberg-warehouse/db/users/metadata/14d4221a-bbc8-4c90-b1c2-d04c307dcf5a-m0.avro\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------------+\n",
      "| id|         name|               email|\n",
      "+---+-------------+--------------------+\n",
      "|  1|Alice Updated|alice_updated@exa...|\n",
      "|  3|      Charlie| charlie@example.com|\n",
      "|  1|        Alice|   alice@example.com|\n",
      "|  2|          Bob|     bob@example.com|\n",
      "+---+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "data 읽기 \n",
    "'''\n",
    "df = spark.read.parquet(\"s3a://emr-data-pipeline-test/iceberg-warehouse/db/users/data/\")\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
